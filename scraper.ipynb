{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping news for keyword: Government policy\n",
      "Fetching URL: https://edition.cnn.com/search?q=Government+policy&from=0&size=10&page=1&sort=newest&types=all&section=\n",
      "Fetching URL: https://edition.cnn.com/search?q=Government+policy&from=0&size=10&page=2&sort=newest&types=all&section=\n",
      "Finished scraping 20 articles for keyword: Government policy\n",
      "Data successfully saved to scraped_cnn_news_trial.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Chrome WebDriver\n",
    "chromedriver_path = 'C://chromedriver-win64/chromedriver.exe'\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Use only one keyword for trial\n",
    "keywords = ['Government policy']\n",
    "\n",
    "# Base CNN URL template\n",
    "base_url = 'https://edition.cnn.com/search?q={}&from=0&size=10&page={}&sort=newest&types=all&section='\n",
    "\n",
    "# Define the date limit\n",
    "date_limit = datetime(2024, 1, 1)\n",
    "\n",
    "# Function to clean up and format keyword for URL\n",
    "def format_keyword(keyword):\n",
    "    return '+'.join(keyword.split())\n",
    "\n",
    "# Function to convert scraped date string to a datetime object\n",
    "def parse_date(date_string):\n",
    "    try:\n",
    "        return datetime.strptime(date_string, '%b %d, %Y')\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Function to scrape news for each keyword\n",
    "def scrape_news(driver, keyword):\n",
    "    formatted_keyword = format_keyword(keyword)\n",
    "    page = 1\n",
    "    scraped_data = []\n",
    "    stop_scraping = False\n",
    "\n",
    "    # Scrape only two pages for trial\n",
    "    while page <= 2 and not stop_scraping:\n",
    "        url = base_url.format(formatted_keyword, page)\n",
    "        print(f\"Fetching URL: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        try:\n",
    "            # Wait for the first headline element to appear\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div[1]/a[2]'))\n",
    "            )\n",
    "\n",
    "            # Scrape link, headline, description, and date using updated XPaths\n",
    "            for i in range(1, 11):  # Scrape up to 10 results per page\n",
    "                try:\n",
    "                    link_elem = driver.find_element(By.XPATH, f'//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div[{i}]/a[2]')\n",
    "                    headline_elem = driver.find_element(By.XPATH, f'//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div[{i}]/a[2]/div/div[1]')\n",
    "                    desc_elem = driver.find_element(By.XPATH, f'//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div[{i}]/a[2]/div/div[3]')\n",
    "                    date_elem = driver.find_element(By.XPATH, f'//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div[{i}]/a[2]/div/div[2]')\n",
    "\n",
    "                    link = link_elem.get_attribute('href')\n",
    "                    headline = headline_elem.text\n",
    "                    description = desc_elem.text\n",
    "                    date_str = date_elem.text.strip()\n",
    "                    article_date = parse_date(date_str)\n",
    "\n",
    "                    # If the date is earlier than the limit, stop scraping for this keyword\n",
    "                    if article_date and article_date < date_limit:\n",
    "                        stop_scraping = True\n",
    "                        break\n",
    "\n",
    "                    # Save scraped data\n",
    "                    scraped_data.append({\n",
    "                        'keyword': keyword,\n",
    "                        'title': headline,\n",
    "                        'desc': description,\n",
    "                        'date': date_str,\n",
    "                        'link': link\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error scraping article {i}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading page or finding elements: {e}\")\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(2)  # Wait between requests to avoid being blocked\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "# Scrape news for the single keyword\n",
    "all_news_data = []\n",
    "for keyword in keywords:\n",
    "    print(f\"Scraping news for keyword: {keyword}\")\n",
    "    news_data = scrape_news(driver, keyword)\n",
    "    all_news_data.extend(news_data)\n",
    "    print(f\"Finished scraping {len(news_data)} articles for keyword: {keyword}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save results to CSV\n",
    "csv_file = 'scraped_cnn_news_trial.csv'\n",
    "csv_columns = ['keyword', 'title', 'desc', 'date', 'link']\n",
    "\n",
    "try:\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_news_data)\n",
    "    print(f\"Data successfully saved to {csv_file}\")\n",
    "except IOError:\n",
    "    print(\"I/O error when writing to CSV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
